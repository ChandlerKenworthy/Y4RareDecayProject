{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Cut:\n",
    "    \"\"\"\n",
    "    A generic object that can be used to make cuts on data using some set of selection criteria.\n",
    "    This will enact cuts on a Data object and can return a modified instance of this object. \n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, data, particles):\n",
    "        \"\"\"\n",
    "        Create the Cut object. The cuts must act on some data and so a filepath to the dataset\n",
    "        of interest is required along with a suffix specifying the decay tree. The cut will act\n",
    "        on the same variables for all particles specified within the particles argument.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "            data : Data\n",
    "            An instantiated Data object that points to a specific tuple and decay tree. It should\n",
    "            be unused such that called data.get_data() returns None.\n",
    "\n",
    "            particles : list\n",
    "            A list of particles that are to be considered during the cuts. Be careful not to specify\n",
    "            particles who do not contain features which are required for the cuts you are going to \n",
    "            perform.\n",
    "        \"\"\"\n",
    "\n",
    "        if data.get_data() is None:\n",
    "            self.d = data\n",
    "            self.particles = particles\n",
    "            self.events_cut = 0\n",
    "        else:\n",
    "            raise Exception(\"ERROR: data must be an unused Data object\")\n",
    "    \n",
    "    def get_events_cut(self):\n",
    "        \"\"\"\n",
    "        Return the total number of events cut during all cuts made within this object.\n",
    "        \"\"\"\n",
    "        \n",
    "        return self.events_cut\n",
    "    \n",
    "    def set_events_cut(self, events):\n",
    "        \"\"\"\n",
    "        Add to the total number of events cut.\n",
    "        \"\"\"\n",
    "        \n",
    "        self.events_cut += events\n",
    "\n",
    "    def nbody(self, n):\n",
    "        \"\"\"\n",
    "        Make a cut on these particles data based on the number of bodies that were detected\n",
    "        in the event. \n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        n : int, list\n",
    "            The number of bodies to make a cut around such that only events with n\n",
    "            decay bodies are kept. This can be a list of ints to keep. In essence\n",
    "            for [1, 3] all events with 1 or 3 bodies are retained.\n",
    "        \"\"\"\n",
    "\n",
    "        self.d.get_specific_features([\"totCandidates\"])\n",
    "        # Add the totCandidates feature to the Data objects data\n",
    "        initial_events = len(self.d.get_data())\n",
    "        if type(n) is int:\n",
    "            df = self.d.get_data().loc[self.d.get_data()[\"totCandidates\"] == n]\n",
    "        else:\n",
    "            df = self.d.get_data().loc[self.d.get_data()[\"totCandidates\"] in n]\n",
    "        self.set_events_cut(initial_events - len(df))\n",
    "        self.d.update_data(df)\n",
    "        \n",
    "    def minimal_pT(self, percentage=None, tolerance=None):\n",
    "        \"\"\"\n",
    "        Cut these data based on the vector sum of transverse momentum. This cut considers\n",
    "        that the incoming beam is almost purely travelling along z and as such the total\n",
    "        transverse momentum of all particles must be zero. Only 1 of percentage or\n",
    "        tolerance should be specified.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        percentage : float\n",
    "            A percentage between 0 and 1 which specifies how many events to cut. The \n",
    "            percentage is specified as the percentage of all events to allow through such \n",
    "            that the lowest percentage percent of events are retained.\n",
    "            \n",
    "        tolerance : float\n",
    "            A way of defining the sensitivty of the cut. This is the number of standard\n",
    "            deviations to allow through such that lower is more sensitive. \n",
    "        \"\"\"\n",
    "        \n",
    "        import numpy as np\n",
    "        \n",
    "        print(len(self.d.get_data()))\n",
    "        self.d.get_particle_data(self.particles, ['PX', 'PY'], drop_duplicates=True)\n",
    "        df = self.d.get_data()\n",
    "        print(len(df))\n",
    "        initial_events = len(df)\n",
    "        df['sum_PX'] = df[[particle + \"_PX\" for particle in self.particles]].sum(axis=1)\n",
    "        df['sum_PY'] = df[[particle + \"_PY\" for particle in self.particles]].sum(axis=1)\n",
    "        df['sum_PT'] = np.sqrt(df['sum_PX']**2 + df['sum_PY']**2)\n",
    "        if [percentage, tolerance].count(None) == 2 or [percentage, tolerance].count(None) == 0:\n",
    "            raise Exception(\"ERROR: Only pass either a percentage or a tolerance to cut by\")\n",
    "        if percentage is None:\n",
    "            # Use a tolerance based cut\n",
    "            mean, std = np.mean(df['sum_PT']), np.std(df['sum_PT'])\n",
    "            df = df.loc[(df['sum_PT'] < mean + (tolerance*std)) and (df['sum_PT'] > mean - (tolerance*std))]\n",
    "        else:\n",
    "            # Use a percentage based cut\n",
    "            n_events_to_drop = int(np.round(len(df) - (len(df) * percentage)))\n",
    "            for i in range(n_events_to_drop):\n",
    "                idx = df['sum_PT'].idxmax()\n",
    "                df.drop(idx, axis=0, inplace=True)\n",
    "        self.set_events_cut(initial_events - len(df))\n",
    "        print(\"MINIMAL CALLS UPDATE\")\n",
    "        self.d.update_data(df)\n",
    "\n",
    "    def cut_particle_cone(self, particles, sigma=2):\n",
    "        \"\"\" TODO: THIS ENTIRE THING IS BROKEN JUST REBUILD IT FROM THE BEGINNING\n",
    "        Make a cut on these particles data based on the direction of motion of the particle.\n",
    "        In essence cut around the cone of momentum which contains a certain ratio of the total\n",
    "        particles. The cone is defined by two angles theta and phi. Removes all events from the\n",
    "        background real LHCb data outside sigma levels of significance. \n",
    "        \"\"\"\n",
    "        import numpy as np\n",
    "\n",
    "        removed_events = {}\n",
    "        mc_data = Cut(False, particles).get_particle_data(['PT', 'PZ', 'PY', 'PX'])\n",
    "        cut_around = {}\n",
    "        for particle, pdata in mc_data.items():\n",
    "            # For every particle in the simulated particle data\n",
    "            phi = np.arctan(pdata['PZ']/pdata['PX'])\n",
    "            theta = np.arctan(pdata['PY']/np.sqrt(pdata['PX']**2 + pdata['PZ']**2))\n",
    "            cut_around[particle] = [[np.mean(phi)-(sigma*np.std(phi)), np.mean(phi)+(sigma*np.std(phi))], [np.mean(theta)-(sigma*np.std(theta)), np.mean(theta)+(sigma*np.std(theta))]]\n",
    "            # now we have arrays for phi and theta of these particles\n",
    "        try:\n",
    "            for particle in particles:\n",
    "                initial_events = len(self.data[particle])\n",
    "                self.data[particle][phi] = np.arctan(self.data[particle]['PZ']/self.data[particle]['PX'])\n",
    "                self.data[particle][theta] = np.arctan(self.data[particle]['PY']/np.sqrt(self.data[particle]['PX']**2 + self.data[particle]['PZ']**2))\n",
    "                # Make the phi cut\n",
    "                self.data[particle] = self.data[particle].loc[(self.data[particle][phi] >= cut_around[particle][0][0]) and (self.data[particle][phi] <= cut_around[particle][0][1])]\n",
    "                # Make the theta cut\n",
    "                self.data[particle] = self.data[particle].loc[(self.data[particle][theta] >= cut_around[particle][1][0]) and (self.data[particle][theta] <= cut_around[particle][1][1])]\n",
    "                final_events = len(self.data[particle])\n",
    "                removed_events[particle] = initial_events - final_events\n",
    "        except:\n",
    "            print(\"ERROR: Have you specified any specific particles to consider?\")\n",
    "\n",
    "        return removed_events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I TRIGGER AS WELL!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "fName, suffix = Consts().get_real_tuple()\n",
    "data = Data(fName, suffix)\n",
    "my_cut = Cut(data, ['L1', 'L2', 'K', 'p'])\n",
    "my_cut.nbody(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12916\n",
      "             totCandidates    L1_PX    L2_PX    K_PX     p_PX    L1_PY  \\\n",
      "eventNumber                                                              \n",
      "19946                    4  -961.08   185.92 -260.21  -179.48  4281.79   \n",
      "19946                    4  -961.08   185.92 -179.48  -510.33  4281.79   \n",
      "19946                    4  -961.08   185.92 -260.21  -179.48  4281.79   \n",
      "19946                    4  -961.08   185.92 -179.48  -510.33  4281.79   \n",
      "489775                   4  4317.10  1993.28  553.12  1524.30  3562.75   \n",
      "\n",
      "              L2_PY    K_PY    p_PY  \n",
      "eventNumber                          \n",
      "19946        991.68  443.11  176.13  \n",
      "19946        991.68  176.13  862.83  \n",
      "19946        991.68  443.11  176.13  \n",
      "19946        991.68  176.13  862.83  \n",
      "489775      -127.43  189.80  506.77  \n",
      "I TRIGGER AS WELL!\n",
      "26980\n",
      "MINIMAL CALLS UPDATE\n",
      "I TRIGGER AS WELL!\n"
     ]
    }
   ],
   "source": [
    "my_cut.minimal_pT(percentage=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[4]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(data.get_data()['totCandidates'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16836"
      ]
     },
     "execution_count": 223,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data.get_data())"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "9ca4116ab2c7470d45496f3104a2f3648c94db730a0039ceabb3a0fb6fe42410"
  },
  "kernelspec": {
   "display_name": "ROOT C++ (Python 3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
