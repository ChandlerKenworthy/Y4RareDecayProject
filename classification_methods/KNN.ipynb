{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning Classification Methods\n",
    "This notebook will use a variety of other more basic classification methods using machine learning. We will then be able to evaluate these against the neural network earlier constructed. We can then evaluate the probability distributions for test precitions, the AUC-ROC cruves and the Punzi FOM value as a function of probability cut. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from matplotlib.ticker import AutoMinorLocator\n",
    "import matplotlib.pyplot as plt\n",
    "import xgboost as xgb\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import sys\n",
    "sys.path.append('../')\n",
    "from dataflow import Flow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "version = '6.0.3'\n",
    "data = Flow(None, None, None, csv_path=f'../data_files/{version}.csv')\n",
    "(X_train, y_train), (X_val, y_val), (X_test, y_test) = data.get_train_val_test_split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_metrics(model, use_test=False):\n",
    "    \"\"\"\n",
    "    Get a lot of useful metrics from the fit model which will now\n",
    "    be tested on the validation data can convert to test if needed\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    model : object\n",
    "        A particular instance of some model which takes the usual\n",
    "        predict and predict_proba and fit methods\n",
    "    \"\"\"\n",
    "    \n",
    "    X = X_val   \n",
    "    Y = y_val \n",
    "    if use_test:\n",
    "        X = X_test\n",
    "        Y = y_test\n",
    "    Y = Y.to_numpy()\n",
    "        \n",
    "    predicted_probabilities = model.predict_proba(X)\n",
    "    # Generate model predictions on these input data e.g. event 1 [prob sig, prob bg]\n",
    "    \n",
    "    pred_signal_probs = predicted_probabilities[:,0]\n",
    "    pred_bg_probs = predicted_probabilities[:,1]\n",
    "    \n",
    "    bins = np.linspace(0, 1, 11)\n",
    "    # Generate 11 bin edges i.e. 10 bins\n",
    "    probability_distribution_s, _ = np.histogram(pred_signal_probs, bins=bins, density=True)\n",
    "    probability_distribution_b, _ = np.histogram(pred_bg_probs, bins=bins, density=True)\n",
    "    prob_dist_with_bins = [[probability_distribution_s, bins], [probability_distribution_b, bins]]\n",
    "    # Get the frequencies and bin edges and dump them into a list\n",
    "\n",
    "    predicted_classes = model.predict(X)\n",
    "    # Sort via a 0.5 cut point these predictions into predicted classes\n",
    "    n_correct = np.count_nonzero((predicted_classes == Y))\n",
    "    binary_accuracy = n_correct / len(predicted_classes)\n",
    "    \n",
    "    return prob_dist_with_bins, binary_accuracy\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KNN Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KNeighborsClassifier(n_neighbors=50)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "knn_classifier = KNeighborsClassifier(n_neighbors=50)\n",
    "knn_classifier.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Binary Accuracy: 0.91921\n"
     ]
    }
   ],
   "source": [
    "probs, bin_acc = get_metrics(knn_classifier)\n",
    "print(f'Validation Binary Accuracy: {bin_acc:.5f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['models/KNN_6.0.3.joblib']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from joblib import dump\n",
    "dump(knn_classifier, 'models/KNN_6.0.3.joblib')                    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XGBoost + RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13:47:03] WARNING: ../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    }
   ],
   "source": [
    "params = {\"objective\": \"binary:logistic\",\n",
    "          \"eta\": 0.3,\n",
    "          \"max_depth\": 6,\n",
    "          \"min_child_weight\": 3,\n",
    "          \"subsample\": 0.7,\n",
    "          \"colsample_bytree\": 0.7,\n",
    "          \"learning_rate\": 0.05,\n",
    "          \"seed\": 1}\n",
    "\n",
    "rf = RandomForestClassifier(n_estimators=120, random_state=1)\n",
    "rf.fit(X_train, y_train)\n",
    "\n",
    "num_trees=250\n",
    "gbm = xgb.train(params, xgb.DMatrix(X_train, y_train), num_trees)\n",
    "\n",
    "test_probs = (rf.predict_proba(X_val)[:,1] + gbm.predict(xgb.DMatrix(X_val)))/2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Binary Accuracy: 0.94298\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the predictions now...\n",
    "classes = np.where(test_probs > 0.5, 1, 0)\n",
    "value_counts = np.count_nonzero(classes == y_val)\n",
    "print(f'Binary Accuracy: {value_counts/len(classes):.5f}')"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "9ca4116ab2c7470d45496f3104a2f3648c94db730a0039ceabb3a0fb6fe42410"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('linux-64')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
